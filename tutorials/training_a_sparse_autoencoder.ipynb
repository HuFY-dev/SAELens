{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A very basic SAE Training Tutorial\n",
    "\n",
    "Please note that it is very easy for tutorial code to go stale so please have a low bar for raising an issue in the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
    "from sae_lens.training.lm_runner import language_model_sae_runner\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Evaluation (Feel Free to Skip)\n",
    "\n",
    "We'll use the runner to train an SAE on a TinyStories Model. This is a very small model so we can train an SAE on it quite quickly. Before we get started, let's load in the model with `transformer_lens` and see what it can do. \n",
    "\n",
    "TransformerLens gives us 2 functions that are useful here (and circuits viz provides a third):\n",
    "1. `transformer_lens.utils.test_prompt` will help us see when the model can infer one token.\n",
    "2. `HookedTransformer.generate` will help us see what happens when we sample from the model.\n",
    "3. `circuitsvis.logits.token_log_probs` will help us visualize the log probs of tokens at several positions in a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8da1c97504448bf9f58b9089da95579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683c4b290b3549faa5e775b515953bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hufy/SAELens/.venv/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23fca5d42ad5469589a18ece23bfef39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2337948e5015418da014404ad5bd3e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2bbcf2601a4c77aa744d5cd068d388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c17c0246c94083ba80cff79b0dfaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee490760c764518ab1dec063b5b93e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-1L-21M into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"tiny-stories-1L-21M\"\n",
    ")  # This will wrap huggingface models and has lots of nice utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a vibe for a model using `model.generate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by generating some stories using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a bear named Bill. He had two bald brothers - Sam and Max. Max was not very fluffy and he was often scared of something that was just his own.\\nOne day, Jack gave his brother were eating and he noticed that'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was an incredible angel. Sometimes the angel would talk to her. One day the angel was playing with her friends.\\n\\nThe angel said, \"Don\\'t worry. You will love to protect the children. Angels are around here for to'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there was a lonely hero who had been on a quest. He was looking for someone to marry, no one.\\n\\nHe lived in the hot castle. He looked outside to see a little woman, and he knew she must stay indoors.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, there were two friends, Bob and Sam. They were Sam, who always shared and were the same. The old man often refused this time.\\n\\nOne day, Bob got a fight with another man. This man was commanded for his family'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Once upon a time, there was a wet forest. Inside her temple was a pretty picture. She wanted to fix the temple, but was scared she wouldn't leave. So, when the temple was dry, she grabbed an umbrella and went to replace it. She wiped\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here we use generate to get 10 completeions with temperature 1. Feel free to play with the prompt to make it more interesting.\n",
    "for i in range(5):\n",
    "    display(\n",
    "        model.generate(\n",
    "            \"Once upon a time\",\n",
    "            stop_at_eos=False,  # avoids a bug on MPS\n",
    "            temperature=1,\n",
    "            verbose=False,\n",
    "            max_new_tokens=50,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we notice is that the model seems to be able to repeat the name of the main character very consistently. It can output a pronoun intead but in some stories will repeat the protagonists name. This seems like an interesting capability to analyse with SAEs. To better understand the models ability to remember the protagonists name, let's extract a prompt where the next character is determined and use the \"test_prompt\" utility from TransformerLens to check the ranking of the token for that name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot checking model abilities with `transformer_lens.utils.test_prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<|endoftext|>', 'Once', ' upon', ' a', ' time', ',', ' there', ' was', ' a', ' little', ' girl', ' named', ' Lily', '.', ' She', ' lived', ' in', ' a', ' big', ',', ' happy', ' little', ' girl', '.', ' On', ' her', ' big', ' adventure', ',']\n",
      "Tokenized answer: [' Lily']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">18.81</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.46</span><span style=\"font-weight: bold\">% Token: | Lily|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m18.81\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m13.46\u001b[0m\u001b[1m% Token: | Lily|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 20.48 Prob: 71.06% Token: | she|\n",
      "Top 1th token. Logit: 18.81 Prob: 13.46% Token: | Lily|\n",
      "Top 2th token. Logit: 17.35 Prob:  3.11% Token: | the|\n",
      "Top 3th token. Logit: 17.26 Prob:  2.86% Token: | her|\n",
      "Top 4th token. Logit: 16.74 Prob:  1.70% Token: | there|\n",
      "Top 5th token. Logit: 16.43 Prob:  1.25% Token: | they|\n",
      "Top 6th token. Logit: 15.80 Prob:  0.66% Token: | all|\n",
      "Top 7th token. Logit: 15.64 Prob:  0.56% Token: | things|\n",
      "Top 8th token. Logit: 15.28 Prob:  0.39% Token: | one|\n",
      "Top 9th token. Logit: 15.24 Prob:  0.38% Token: | lived|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' Lily'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' Lily'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformer_lens.utils import test_prompt\n",
    "\n",
    "# Test the model with a prompt\n",
    "test_prompt(\n",
    "    \"Once upon a time, there was a little girl named Lily. She lived in a big, happy little girl. On her big adventure,\",\n",
    "    \" Lily\",\n",
    "    model,\n",
    "    prepend_space_to_answer=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output above, we see that the model assigns ~ 70% probability to \"she\" being the next token, and a 13% chance to \" Lily\" being the next token. Other names like Lucy or Anna are not highly ranked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Model Capabilities with Log Probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at token ranking for a single prompt is interesting, but a much higher through way to understand models is to look at token log probs for all tokens in text. We can use the `circuits_vis` package to get a nice visualization where we can see tokenization, and hover to get the top5 tokens by log probability. Darker tokens are tokens where the model assigned a higher probability to the actual next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'circuitsvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcircuitsvis\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mcv\u001b[39;00m  \u001b[38;5;66;03m# optional dep, install with pip install circuitsvis\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Let's make a longer prompt and see the log probabilities of the tokens\u001b[39;00m\n\u001b[1;32m      4\u001b[0m example_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mHi, how are you doing this? I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm really enjoying your posts\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'circuitsvis'"
     ]
    }
   ],
   "source": [
    "import circuitsvis as cv  # optional dep, install with pip install circuitsvis\n",
    "\n",
    "# Let's make a longer prompt and see the log probabilities of the tokens\n",
    "example_prompt = \"\"\"Hi, how are you doing this? I'm really enjoying your posts\"\"\"\n",
    "logits, cache = model.run_with_cache(example_prompt)\n",
    "cv.logits.token_log_probs(\n",
    "    model.to_tokens(example_prompt),\n",
    "    model(example_prompt)[0].log_softmax(dim=-1),\n",
    "    model.to_string,\n",
    ")\n",
    "# hover on the output to see the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine `model.generate` and the token log probs visualization to see the log probs on text generated by the model. Note that we can play with the temperature and this should sample less likely trajectories according to the model. I've increased the maximum number of tokens in order to get a full story.\n",
    "\n",
    "Some things to explore:\n",
    "- Which tokens does the model assign high probability to? Can you see how the model should know which word comes next?\n",
    "- What happens if you increase / decrease the temperature?\n",
    "- Do the rankings of tokens seem sensible to you? What about where the model doesn't assign a high probability to the token which came next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = model.generate(\n",
    "    \"Once upon a time\",\n",
    "    stop_at_eos=False,  # avoids a bug on MPS\n",
    "    temperature=1,\n",
    "    verbose=True,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "logits, cache = model.run_with_cache(example_prompt)\n",
    "cv.logits.token_log_probs(\n",
    "    model.to_tokens(example_prompt),\n",
    "    model(example_prompt)[0].log_softmax(dim=-1),\n",
    "    model.to_string,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an SAE\n",
    "\n",
    "Now we're ready to train out SAE. We'll make a runner config, instantiate the runner and the rest is taken care of for us!\n",
    "\n",
    "During training, you use weights and biases to check key metrics which indicate how well we are able to optimize the variables we care about.\n",
    "\n",
    "To get a better sense of which variables to look at, you can read my (Joseph's) post [here](https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream) and especially look at my weights and biases report [here](https://links-cdn.wandb.ai/wandb-public-images/links/jbloom/uue9i416.html).\n",
    "\n",
    "A few tips:\n",
    "- Feel free to reorganize your wandb dashboard to put L0, CE_Loss_score, explained variance and other key metrics in one section at the top.\n",
    "- Make a [run comparer](https://docs.wandb.ai/guides/app/features/panels/run-comparer) when tuning hyperparameters.\n",
    "- You can download the resulting sparse autoencoder / sparsity estimate from wandb and upload them to huggingface if you want to share your SAE with other.\n",
    "    - cfg.json (training config)\n",
    "    - sae_weight.safetensors (model weights)\n",
    "    - sparsity.safetensors (sparsity estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Out\n",
    "\n",
    "I've tuned the hyperparameters below for a decent SAE which achieves 93% CE Loss recovered and an L0 of ~60, and runs in about 22 minutes on an A100 (sorry peeps, that is the kinda hardware you'll want to be using moving on). I think you could hyperparameter tune toward a much better SAE in the future (and would welcome PR's that improve on the hyperparameters here!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 16384-L1-0.001-LR-0.0008-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 12207\n",
      "Total wandb updates: 1220\n",
      "n_tokens_per_feature_sampling_window (millions): 2097.152\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 12 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hufy/SAELens/.venv/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model tiny-stories-1L-21M into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Run name: 16384-L1-0.001-LR-0.0008-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 12207\n",
      "Total wandb updates: 1220\n",
      "n_tokens_per_feature_sampling_window (millions): 2097.152\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 12 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n",
      "Run name: 16384-L1-0.001-LR-0.0008-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.524288\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 12207\n",
      "Total wandb updates: 1220\n",
      "n_tokens_per_feature_sampling_window (millions): 2097.152\n",
      "n_tokens_per_dead_feature_window (millions): 2097.152\n",
      "We will reset the sparsity calculation 12 times.\n",
      "Number tokens in sparsity calculation window: 4.10e+06\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:x4a7nc5i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a908c2514d433fa810cb0a3e55f011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate_layer0</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/current_learning_rate_layer1</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/current_learning_rate_layer10</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/current_learning_rate_layer11</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/current_learning_rate_layer2</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/current_learning_rate_layer3</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/current_learning_rate_layer4</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/current_learning_rate_layer5</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/current_learning_rate_layer6</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/current_learning_rate_layer7</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/current_learning_rate_layer8</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/current_learning_rate_layer9</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>losses/ghost_grad_loss_layer0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_layer1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_layer10</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_layer11</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_layer2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_layer3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_layer4</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_layer5</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_layer6</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_layer7</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_layer8</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/ghost_grad_loss_layer9</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss_layer0</td><td>▆▆█▇▅▆▆▅▆▄▅▅▅▆▄▄▃▅▃▁</td></tr><tr><td>losses/l1_loss_layer1</td><td>▃▃█▅▄▄▃▂▄▁▂▂▂▄▄▂▃▃▃▂</td></tr><tr><td>losses/l1_loss_layer10</td><td>▇█▇▆▅▅█▄▇▅▆▅▇▅▆▂▁▃▂▃</td></tr><tr><td>losses/l1_loss_layer11</td><td>▄▇▇▆▆▂▆▄█▁█▄▆▅▆▄▄▁▂▃</td></tr><tr><td>losses/l1_loss_layer2</td><td>▁▄█▄▄▂▃▂▄▁▃▂▂▄▄▁▂▂▃▃</td></tr><tr><td>losses/l1_loss_layer3</td><td>▃▅█▅▃▃▄▂▅▁▃▃▂▄▅▂▂▂▄▃</td></tr><tr><td>losses/l1_loss_layer4</td><td>▃▄█▃▄▃▃▁▄▁▂▁▃▃▃▂▂▂▂▁</td></tr><tr><td>losses/l1_loss_layer5</td><td>▂▅█▅▅▅▅▂▅▂▃▃▄▆▅▄▅▃▄▁</td></tr><tr><td>losses/l1_loss_layer6</td><td>▇▇██▅▇▅▃▅▂▄▄▁▆▇▁▃▃▃▁</td></tr><tr><td>losses/l1_loss_layer7</td><td>▆▇▄█▆▄▅▁▆▆▄▄▆▄▄▁▃▃▃▃</td></tr><tr><td>losses/l1_loss_layer8</td><td>▇█▃▇▆▆▆▄▃▄▄█▄▃▅▃▃▃▃▁</td></tr><tr><td>losses/l1_loss_layer9</td><td>██▃▅▅▅▆▄▃▅▅▄▄▄▅▁▃▃▄▂</td></tr><tr><td>losses/mse_loss_layer0</td><td>▃▄█▅▄▃▃▁▄▁▂▁▂▃▃▁▂▂▂▁</td></tr><tr><td>losses/mse_loss_layer1</td><td>▂▃█▄▄▃▃▁▄▁▂▂▂▄▄▂▃▂▄▃</td></tr><tr><td>losses/mse_loss_layer10</td><td>██▆▇▅▅▇▅▆▄▅▄▅▃▄▂▁▁▁▂</td></tr><tr><td>losses/mse_loss_layer11</td><td>▂█▇▅▄▄▄▆▄▁▇▅▄▃▆▄▂▁▁▂</td></tr><tr><td>losses/mse_loss_layer2</td><td>▂▃█▄▄▃▃▁▄▁▂▂▂▄▄▂▃▂▄▃</td></tr><tr><td>losses/mse_loss_layer3</td><td>▂▄█▄▄▃▃▁▄▁▂▂▂▄▄▂▂▂▃▂</td></tr><tr><td>losses/mse_loss_layer4</td><td>▃▄█▅▄▃▃▁▄▁▂▂▂▃▃▂▂▂▃▂</td></tr><tr><td>losses/mse_loss_layer5</td><td>▅▆█▆▅▄▄▂▄▂▂▂▂▃▃▁▂▂▂▁</td></tr><tr><td>losses/mse_loss_layer6</td><td>▇███▇▆▆▄▅▄▄▃▃▄▃▂▂▂▂▁</td></tr><tr><td>losses/mse_loss_layer7</td><td>██▇█▇▆▆▄▅▄▄▃▃▄▂▂▂▂▂▁</td></tr><tr><td>losses/mse_loss_layer8</td><td>██▆▇▇▆▆▅▄▅▄▄▄▃▃▂▂▂▂▁</td></tr><tr><td>losses/mse_loss_layer9</td><td>▇█▄▆▅▅▆▄▄▄▄▄▃▂▃▁▂▂▂▁</td></tr><tr><td>losses/overall_loss_layer0</td><td>▃▄█▅▄▃▃▁▄▁▂▁▂▃▃▁▁▂▂▁</td></tr><tr><td>losses/overall_loss_layer1</td><td>▂▃█▄▄▃▃▁▄▁▂▂▂▄▄▂▃▂▄▃</td></tr><tr><td>losses/overall_loss_layer10</td><td>██▆▇▅▅▇▅▆▄▅▄▅▃▄▂▁▁▁▂</td></tr><tr><td>losses/overall_loss_layer11</td><td>▂█▇▅▄▄▄▆▄▁▇▅▄▃▆▄▂▁▁▂</td></tr><tr><td>losses/overall_loss_layer2</td><td>▂▃█▄▄▃▃▁▄▁▂▂▂▄▄▂▃▂▄▃</td></tr><tr><td>losses/overall_loss_layer3</td><td>▂▄█▄▄▃▃▁▄▁▂▂▂▄▄▂▂▂▃▂</td></tr><tr><td>losses/overall_loss_layer4</td><td>▃▄█▅▄▃▃▁▄▁▂▂▂▃▃▂▂▂▃▂</td></tr><tr><td>losses/overall_loss_layer5</td><td>▅▆█▆▅▄▄▂▄▂▂▂▂▃▃▁▂▂▂▁</td></tr><tr><td>losses/overall_loss_layer6</td><td>▇███▇▆▆▄▅▄▄▃▃▄▃▂▂▂▂▁</td></tr><tr><td>losses/overall_loss_layer7</td><td>██▇█▇▆▆▄▅▄▄▃▃▄▂▂▂▂▂▁</td></tr><tr><td>losses/overall_loss_layer8</td><td>██▆▇▇▆▆▅▄▅▄▅▄▃▃▂▂▂▂▁</td></tr><tr><td>losses/overall_loss_layer9</td><td>▇█▄▆▅▅▆▄▄▄▄▄▃▂▃▁▂▂▂▁</td></tr><tr><td>metrics/CE_loss_score_layer0</td><td>▁█</td></tr><tr><td>metrics/CE_loss_score_layer1</td><td>▁█</td></tr><tr><td>metrics/CE_loss_score_layer10</td><td>▁█</td></tr><tr><td>metrics/CE_loss_score_layer11</td><td>▁</td></tr><tr><td>metrics/CE_loss_score_layer2</td><td>▁█</td></tr><tr><td>metrics/CE_loss_score_layer3</td><td>▁█</td></tr><tr><td>metrics/CE_loss_score_layer4</td><td>▁█</td></tr><tr><td>metrics/CE_loss_score_layer5</td><td>▁█</td></tr><tr><td>metrics/CE_loss_score_layer6</td><td>▁█</td></tr><tr><td>metrics/CE_loss_score_layer7</td><td>▁█</td></tr><tr><td>metrics/CE_loss_score_layer8</td><td>▁█</td></tr><tr><td>metrics/CE_loss_score_layer9</td><td>▁█</td></tr><tr><td>metrics/ce_loss_with_ablation_layer0</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_ablation_layer1</td><td>▁█</td></tr><tr><td>metrics/ce_loss_with_ablation_layer10</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_ablation_layer11</td><td>▁</td></tr><tr><td>metrics/ce_loss_with_ablation_layer2</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_ablation_layer3</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_ablation_layer4</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_ablation_layer5</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_ablation_layer6</td><td>▁█</td></tr><tr><td>metrics/ce_loss_with_ablation_layer7</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_ablation_layer8</td><td>▁█</td></tr><tr><td>metrics/ce_loss_with_ablation_layer9</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_sae_layer0</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_sae_layer1</td><td>▁█</td></tr><tr><td>metrics/ce_loss_with_sae_layer10</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_sae_layer11</td><td>▁</td></tr><tr><td>metrics/ce_loss_with_sae_layer2</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_sae_layer3</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_sae_layer4</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_sae_layer5</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_sae_layer6</td><td>▁█</td></tr><tr><td>metrics/ce_loss_with_sae_layer7</td><td>█▁</td></tr><tr><td>metrics/ce_loss_with_sae_layer8</td><td>▁█</td></tr><tr><td>metrics/ce_loss_with_sae_layer9</td><td>█▁</td></tr><tr><td>metrics/ce_loss_without_sae_layer0</td><td>█▁</td></tr><tr><td>metrics/ce_loss_without_sae_layer1</td><td>▁█</td></tr><tr><td>metrics/ce_loss_without_sae_layer10</td><td>█▁</td></tr><tr><td>metrics/ce_loss_without_sae_layer11</td><td>▁</td></tr><tr><td>metrics/ce_loss_without_sae_layer2</td><td>█▁</td></tr><tr><td>metrics/ce_loss_without_sae_layer3</td><td>█▁</td></tr><tr><td>metrics/ce_loss_without_sae_layer4</td><td>█▁</td></tr><tr><td>metrics/ce_loss_without_sae_layer5</td><td>█▁</td></tr><tr><td>metrics/ce_loss_without_sae_layer6</td><td>▁█</td></tr><tr><td>metrics/ce_loss_without_sae_layer7</td><td>█▁</td></tr><tr><td>metrics/ce_loss_without_sae_layer8</td><td>▁█</td></tr><tr><td>metrics/ce_loss_without_sae_layer9</td><td>█▁</td></tr><tr><td>metrics/explained_variance_layer0</td><td>▁▁▁▂▂▃▃▄▄▅▆▆▆▇▇▇▇███</td></tr><tr><td>metrics/explained_variance_layer1</td><td>▁▂▆▄▃▄▄▄▅▅▆▆▆▇█▇▇███</td></tr><tr><td>metrics/explained_variance_layer10</td><td>▁▁▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>metrics/explained_variance_layer11</td><td>▁▄▃▂▂▂▃▄▄▃▆▅▆▅▇▇▆▆▇█</td></tr><tr><td>metrics/explained_variance_layer2</td><td>▁▄█▅▅▃▃▁▆▂▃▃▄▆▆▄▅▄▆▅</td></tr><tr><td>metrics/explained_variance_layer3</td><td>▁▁▂▂▂▃▃▄▄▅▆▆▇▇▇▇████</td></tr><tr><td>metrics/explained_variance_layer4</td><td>▁▁▂▂▂▃▄▄▅▅▆▆▇▇▇▇████</td></tr><tr><td>metrics/explained_variance_layer5</td><td>▁▂▂▂▂▃▄▄▅▅▆▆▇▇▇▇████</td></tr><tr><td>metrics/explained_variance_layer6</td><td>▁▁▁▂▂▃▄▄▅▅▆▆▆▇▇▇████</td></tr><tr><td>metrics/explained_variance_layer7</td><td>▁▁▁▂▃▃▄▄▄▅▅▆▆▇▇▇████</td></tr><tr><td>metrics/explained_variance_layer8</td><td>▁▁▁▂▂▃▃▄▄▅▅▆▆▆▇▇▇███</td></tr><tr><td>metrics/explained_variance_layer9</td><td>▁▁▁▂▂▂▃▃▃▄▄▅▆▆▇▇▇███</td></tr><tr><td>metrics/explained_variance_std_layer0</td><td>█▇█▆▆▅▆▅▅▄▃▃▃▃▂▂▃▁▂▂</td></tr><tr><td>metrics/explained_variance_std_layer1</td><td>█▆▁▅▅▅▅▆▃▅▄▃▃▂▁▂▂▂▁▂</td></tr><tr><td>metrics/explained_variance_std_layer10</td><td>▆▆▆▃▆█▇▁▃▄▃▃▅▇▅▃▃▇▆▃</td></tr><tr><td>metrics/explained_variance_std_layer11</td><td>▃▆█▄▆▃▅▄▅▄▇▄▄▄▆▄▂▁▂▃</td></tr><tr><td>metrics/explained_variance_std_layer2</td><td>█▅▁▃▄▅▅█▃▇▅▅▄▃▂▄▃▃▂▃</td></tr><tr><td>metrics/explained_variance_std_layer3</td><td>█▇▇▆▆▅▆▄▅▄▃▃▂▂▁▁▂▁▁▁</td></tr><tr><td>metrics/explained_variance_std_layer4</td><td>█▇▇▆▆▅▅▅▅▄▃▃▂▂▁▁▂▁▁▁</td></tr><tr><td>metrics/explained_variance_std_layer5</td><td>█▇▇▆▆▅▅▄▅▄▃▂▂▂▁▁▂▁▁▁</td></tr><tr><td>metrics/explained_variance_std_layer6</td><td>███▆▆▅▅▄▅▄▃▂▃▂▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance_std_layer7</td><td>██▇▇▆▆▅▅▄▄▃▂▂▂▁▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance_std_layer8</td><td>███▆▆▅▅▄▅▄▃▃▃▂▁▂▂▁▁▁</td></tr><tr><td>metrics/explained_variance_std_layer9</td><td>▆▆█▇▆▅▄▅▅▃▃▂▂▂▂▁▁▁▁▂</td></tr><tr><td>metrics/l0_layer0</td><td>██▆▇▆▇▇▆▆▆▅▅▄▅▄▄▄▄▂▁</td></tr><tr><td>metrics/l0_layer1</td><td>█▇████▄█▄▄▃▃▃▆▃▃▃▄▃▁</td></tr><tr><td>metrics/l0_layer10</td><td>▇▅█▆▅▆▇▇▆▆▄▆▅▂▄▃▄▆▁▂</td></tr><tr><td>metrics/l0_layer11</td><td>▄▅▅▇▄▄▄█▆▃▄▅▅▂▁▄▁▅▂▂</td></tr><tr><td>metrics/l0_layer2</td><td>▅█▆▄▇▄▆█▄▄▇▄▂▄▂▁▃▃▂▄</td></tr><tr><td>metrics/l0_layer3</td><td>██▆▇▄▆▇▆█▄▅▆▄▃▇▁▁▂▄▃</td></tr><tr><td>metrics/l0_layer4</td><td>▆▇█▄▆▇▅▆▅▆▄▄▅▅▄▆▁▄▃▃</td></tr><tr><td>metrics/l0_layer5</td><td>▃▆▅▅█▇▇▆▅▄▅▄▅▄▆▇▇▅▅▁</td></tr><tr><td>metrics/l0_layer6</td><td>▇▅▇▅▅█▅▆▄▄▅▄▁▄▆▂▄▅▃▃</td></tr><tr><td>metrics/l0_layer7</td><td>▅▇▃█▇▄▃▃▆▆▄▄▅▂▆▁▄▂▁▅</td></tr><tr><td>metrics/l0_layer8</td><td>▇█▅▅▅▆▄█▅▃▄▇▅▃▅▄▂▂▃▁</td></tr><tr><td>metrics/l0_layer9</td><td>█▂▆▅▆▆▆▄▃▄▄▁▅▂▆▂▂▃▁▁</td></tr><tr><td>metrics/l2_norm_layer0</td><td>█▁</td></tr><tr><td>metrics/l2_norm_layer1</td><td>█▁</td></tr><tr><td>metrics/l2_norm_layer10</td><td>█▁</td></tr><tr><td>metrics/l2_norm_layer11</td><td>▁</td></tr><tr><td>metrics/l2_norm_layer2</td><td>█▁</td></tr><tr><td>metrics/l2_norm_layer3</td><td>█▁</td></tr><tr><td>metrics/l2_norm_layer4</td><td>█▁</td></tr><tr><td>metrics/l2_norm_layer5</td><td>█▁</td></tr><tr><td>metrics/l2_norm_layer6</td><td>█▁</td></tr><tr><td>metrics/l2_norm_layer7</td><td>█▁</td></tr><tr><td>metrics/l2_norm_layer8</td><td>█▁</td></tr><tr><td>metrics/l2_norm_layer9</td><td>█▁</td></tr><tr><td>metrics/l2_ratio_layer0</td><td>█▁</td></tr><tr><td>metrics/l2_ratio_layer1</td><td>█▁</td></tr><tr><td>metrics/l2_ratio_layer10</td><td>█▁</td></tr><tr><td>metrics/l2_ratio_layer11</td><td>▁</td></tr><tr><td>metrics/l2_ratio_layer2</td><td>█▁</td></tr><tr><td>metrics/l2_ratio_layer3</td><td>█▁</td></tr><tr><td>metrics/l2_ratio_layer4</td><td>█▁</td></tr><tr><td>metrics/l2_ratio_layer5</td><td>█▁</td></tr><tr><td>metrics/l2_ratio_layer6</td><td>█▁</td></tr><tr><td>metrics/l2_ratio_layer7</td><td>█▁</td></tr><tr><td>metrics/l2_ratio_layer8</td><td>█▁</td></tr><tr><td>metrics/l2_ratio_layer9</td><td>█▁</td></tr><tr><td>sparsity/dead_features_layer0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_layer1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_layer10</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_layer11</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_layer2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_layer3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_layer4</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_layer5</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_layer6</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_layer7</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_layer8</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/dead_features_layer9</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer10</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer11</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer3</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer4</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer5</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer6</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer7</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer8</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired_layer9</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate_layer0</td><td>0.0</td></tr><tr><td>details/current_learning_rate_layer1</td><td>0.0</td></tr><tr><td>details/current_learning_rate_layer10</td><td>0.0</td></tr><tr><td>details/current_learning_rate_layer11</td><td>0.0</td></tr><tr><td>details/current_learning_rate_layer2</td><td>0.0</td></tr><tr><td>details/current_learning_rate_layer3</td><td>0.0</td></tr><tr><td>details/current_learning_rate_layer4</td><td>0.0</td></tr><tr><td>details/current_learning_rate_layer5</td><td>0.0</td></tr><tr><td>details/current_learning_rate_layer6</td><td>0.0</td></tr><tr><td>details/current_learning_rate_layer7</td><td>0.0</td></tr><tr><td>details/current_learning_rate_layer8</td><td>0.0</td></tr><tr><td>details/current_learning_rate_layer9</td><td>0.0</td></tr><tr><td>details/n_training_tokens</td><td>409600</td></tr><tr><td>losses/ghost_grad_loss_layer0</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_layer1</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_layer10</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_layer11</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_layer2</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_layer3</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_layer4</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_layer5</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_layer6</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_layer7</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_layer8</td><td>0.0</td></tr><tr><td>losses/ghost_grad_loss_layer9</td><td>0.0</td></tr><tr><td>losses/l1_loss_layer0</td><td>3480.09968</td></tr><tr><td>losses/l1_loss_layer1</td><td>3438.17782</td></tr><tr><td>losses/l1_loss_layer10</td><td>3759.06754</td></tr><tr><td>losses/l1_loss_layer11</td><td>3980.97634</td></tr><tr><td>losses/l1_loss_layer2</td><td>3443.69411</td></tr><tr><td>losses/l1_loss_layer3</td><td>3447.54744</td></tr><tr><td>losses/l1_loss_layer4</td><td>3448.65584</td></tr><tr><td>losses/l1_loss_layer5</td><td>3457.04508</td></tr><tr><td>losses/l1_loss_layer6</td><td>3471.75074</td></tr><tr><td>losses/l1_loss_layer7</td><td>3495.32819</td></tr><tr><td>losses/l1_loss_layer8</td><td>3532.46474</td></tr><tr><td>losses/l1_loss_layer9</td><td>3600.67797</td></tr><tr><td>losses/mse_loss_layer0</td><td>0.84057</td></tr><tr><td>losses/mse_loss_layer1</td><td>1.40154</td></tr><tr><td>losses/mse_loss_layer10</td><td>3.97329</td></tr><tr><td>losses/mse_loss_layer11</td><td>12.10061</td></tr><tr><td>losses/mse_loss_layer2</td><td>27.58067</td></tr><tr><td>losses/mse_loss_layer3</td><td>0.55623</td></tr><tr><td>losses/mse_loss_layer4</td><td>0.50682</td></tr><tr><td>losses/mse_loss_layer5</td><td>0.53346</td></tr><tr><td>losses/mse_loss_layer6</td><td>0.63356</td></tr><tr><td>losses/mse_loss_layer7</td><td>0.8532</td></tr><tr><td>losses/mse_loss_layer8</td><td>1.22089</td></tr><tr><td>losses/mse_loss_layer9</td><td>1.93925</td></tr><tr><td>losses/overall_loss_layer0</td><td>4.32067</td></tr><tr><td>losses/overall_loss_layer1</td><td>4.83972</td></tr><tr><td>losses/overall_loss_layer10</td><td>7.73236</td></tr><tr><td>losses/overall_loss_layer11</td><td>16.08159</td></tr><tr><td>losses/overall_loss_layer2</td><td>31.02436</td></tr><tr><td>losses/overall_loss_layer3</td><td>4.00378</td></tr><tr><td>losses/overall_loss_layer4</td><td>3.95548</td></tr><tr><td>losses/overall_loss_layer5</td><td>3.9905</td></tr><tr><td>losses/overall_loss_layer6</td><td>4.10531</td></tr><tr><td>losses/overall_loss_layer7</td><td>4.34853</td></tr><tr><td>losses/overall_loss_layer8</td><td>4.75335</td></tr><tr><td>losses/overall_loss_layer9</td><td>5.53993</td></tr><tr><td>metrics/CE_loss_score_layer0</td><td>0.18346</td></tr><tr><td>metrics/CE_loss_score_layer1</td><td>-2.25058</td></tr><tr><td>metrics/CE_loss_score_layer10</td><td>0.33253</td></tr><tr><td>metrics/CE_loss_score_layer11</td><td>-0.41364</td></tr><tr><td>metrics/CE_loss_score_layer2</td><td>-0.95534</td></tr><tr><td>metrics/CE_loss_score_layer3</td><td>-0.3565</td></tr><tr><td>metrics/CE_loss_score_layer4</td><td>-0.28463</td></tr><tr><td>metrics/CE_loss_score_layer5</td><td>-0.17225</td></tr><tr><td>metrics/CE_loss_score_layer6</td><td>-0.03962</td></tr><tr><td>metrics/CE_loss_score_layer7</td><td>-0.05684</td></tr><tr><td>metrics/CE_loss_score_layer8</td><td>0.00213</td></tr><tr><td>metrics/CE_loss_score_layer9</td><td>0.10641</td></tr><tr><td>metrics/ce_loss_with_ablation_layer0</td><td>6.87491</td></tr><tr><td>metrics/ce_loss_with_ablation_layer1</td><td>2.64368</td></tr><tr><td>metrics/ce_loss_with_ablation_layer10</td><td>2.66641</td></tr><tr><td>metrics/ce_loss_with_ablation_layer11</td><td>2.64554</td></tr><tr><td>metrics/ce_loss_with_ablation_layer2</td><td>2.64553</td></tr><tr><td>metrics/ce_loss_with_ablation_layer3</td><td>2.6623</td></tr><tr><td>metrics/ce_loss_with_ablation_layer4</td><td>2.65679</td></tr><tr><td>metrics/ce_loss_with_ablation_layer5</td><td>2.67259</td></tr><tr><td>metrics/ce_loss_with_ablation_layer6</td><td>2.69904</td></tr><tr><td>metrics/ce_loss_with_ablation_layer7</td><td>2.65535</td></tr><tr><td>metrics/ce_loss_with_ablation_layer8</td><td>2.68658</td></tr><tr><td>metrics/ce_loss_with_ablation_layer9</td><td>2.69319</td></tr><tr><td>metrics/ce_loss_with_sae_layer0</td><td>6.08683</td></tr><tr><td>metrics/ce_loss_with_sae_layer1</td><td>2.70168</td></tr><tr><td>metrics/ce_loss_with_sae_layer10</td><td>2.63114</td></tr><tr><td>metrics/ce_loss_with_sae_layer11</td><td>2.66687</td></tr><tr><td>metrics/ce_loss_with_sae_layer2</td><td>2.6876</td></tr><tr><td>metrics/ce_loss_with_sae_layer3</td><td>2.68649</td></tr><tr><td>metrics/ce_loss_with_sae_layer4</td><td>2.67646</td></tr><tr><td>metrics/ce_loss_with_sae_layer5</td><td>2.68502</td></tr><tr><td>metrics/ce_loss_with_sae_layer6</td><td>2.70203</td></tr><tr><td>metrics/ce_loss_with_sae_layer7</td><td>2.65947</td></tr><tr><td>metrics/ce_loss_with_sae_layer8</td><td>2.68633</td></tr><tr><td>metrics/ce_loss_with_sae_layer9</td><td>2.68359</td></tr><tr><td>metrics/ce_loss_without_sae_layer0</td><td>2.60059</td></tr><tr><td>metrics/ce_loss_without_sae_layer1</td><td>2.6172</td></tr><tr><td>metrics/ce_loss_without_sae_layer10</td><td>2.55934</td></tr><tr><td>metrics/ce_loss_without_sae_layer11</td><td>2.59186</td></tr><tr><td>metrics/ce_loss_without_sae_layer2</td><td>2.60117</td></tr><tr><td>metrics/ce_loss_without_sae_layer3</td><td>2.59469</td></tr><tr><td>metrics/ce_loss_without_sae_layer4</td><td>2.58795</td></tr><tr><td>metrics/ce_loss_without_sae_layer5</td><td>2.59984</td></tr><tr><td>metrics/ce_loss_without_sae_layer6</td><td>2.6241</td></tr><tr><td>metrics/ce_loss_without_sae_layer7</td><td>2.58178</td></tr><tr><td>metrics/ce_loss_without_sae_layer8</td><td>2.59956</td></tr><tr><td>metrics/ce_loss_without_sae_layer9</td><td>2.60216</td></tr><tr><td>metrics/explained_variance_layer0</td><td>-0.28334</td></tr><tr><td>metrics/explained_variance_layer1</td><td>-1.20414</td></tr><tr><td>metrics/explained_variance_layer10</td><td>-0.0052</td></tr><tr><td>metrics/explained_variance_layer11</td><td>0.04543</td></tr><tr><td>metrics/explained_variance_layer2</td><td>-0.10681</td></tr><tr><td>metrics/explained_variance_layer3</td><td>-0.65728</td></tr><tr><td>metrics/explained_variance_layer4</td><td>-0.54328</td></tr><tr><td>metrics/explained_variance_layer5</td><td>-0.39221</td></tr><tr><td>metrics/explained_variance_layer6</td><td>-0.26063</td></tr><tr><td>metrics/explained_variance_layer7</td><td>-0.17705</td></tr><tr><td>metrics/explained_variance_layer8</td><td>-0.10853</td></tr><tr><td>metrics/explained_variance_layer9</td><td>-0.05537</td></tr><tr><td>metrics/explained_variance_std_layer0</td><td>0.20961</td></tr><tr><td>metrics/explained_variance_std_layer1</td><td>0.7828</td></tr><tr><td>metrics/explained_variance_std_layer10</td><td>0.03805</td></tr><tr><td>metrics/explained_variance_std_layer11</td><td>0.10076</td></tr><tr><td>metrics/explained_variance_std_layer2</td><td>0.26983</td></tr><tr><td>metrics/explained_variance_std_layer3</td><td>0.36094</td></tr><tr><td>metrics/explained_variance_std_layer4</td><td>0.24475</td></tr><tr><td>metrics/explained_variance_std_layer5</td><td>0.17517</td></tr><tr><td>metrics/explained_variance_std_layer6</td><td>0.09726</td></tr><tr><td>metrics/explained_variance_std_layer7</td><td>0.07252</td></tr><tr><td>metrics/explained_variance_std_layer8</td><td>0.05056</td></tr><tr><td>metrics/explained_variance_std_layer9</td><td>0.04344</td></tr><tr><td>metrics/l0_layer0</td><td>6134.12646</td></tr><tr><td>metrics/l0_layer1</td><td>6139.78564</td></tr><tr><td>metrics/l0_layer10</td><td>6140.90869</td></tr><tr><td>metrics/l0_layer11</td><td>6137.88916</td></tr><tr><td>metrics/l0_layer2</td><td>6142.00195</td></tr><tr><td>metrics/l0_layer3</td><td>6141.24316</td></tr><tr><td>metrics/l0_layer4</td><td>6140.35205</td></tr><tr><td>metrics/l0_layer5</td><td>6139.18945</td></tr><tr><td>metrics/l0_layer6</td><td>6142.5957</td></tr><tr><td>metrics/l0_layer7</td><td>6143.44629</td></tr><tr><td>metrics/l0_layer8</td><td>6140.05127</td></tr><tr><td>metrics/l0_layer9</td><td>6141.08838</td></tr><tr><td>metrics/l2_norm_layer0</td><td>19.15512</td></tr><tr><td>metrics/l2_norm_layer1</td><td>10.10271</td></tr><tr><td>metrics/l2_norm_layer10</td><td>55.84025</td></tr><tr><td>metrics/l2_norm_layer11</td><td>50.49716</td></tr><tr><td>metrics/l2_norm_layer2</td><td>10.07295</td></tr><tr><td>metrics/l2_norm_layer3</td><td>10.11838</td></tr><tr><td>metrics/l2_norm_layer4</td><td>10.06409</td></tr><tr><td>metrics/l2_norm_layer5</td><td>10.78713</td></tr><tr><td>metrics/l2_norm_layer6</td><td>11.1071</td></tr><tr><td>metrics/l2_norm_layer7</td><td>12.45596</td></tr><tr><td>metrics/l2_norm_layer8</td><td>13.52383</td></tr><tr><td>metrics/l2_norm_layer9</td><td>19.90887</td></tr><tr><td>metrics/l2_ratio_layer0</td><td>0.73497</td></tr><tr><td>metrics/l2_ratio_layer1</td><td>1.07324</td></tr><tr><td>metrics/l2_ratio_layer10</td><td>0.78731</td></tr><tr><td>metrics/l2_ratio_layer11</td><td>0.65823</td></tr><tr><td>metrics/l2_ratio_layer2</td><td>0.95499</td></tr><tr><td>metrics/l2_ratio_layer3</td><td>0.79978</td></tr><tr><td>metrics/l2_ratio_layer4</td><td>0.74313</td></tr><tr><td>metrics/l2_ratio_layer5</td><td>0.65854</td></tr><tr><td>metrics/l2_ratio_layer6</td><td>0.56406</td></tr><tr><td>metrics/l2_ratio_layer7</td><td>0.5205</td></tr><tr><td>metrics/l2_ratio_layer8</td><td>0.456</td></tr><tr><td>metrics/l2_ratio_layer9</td><td>0.52058</td></tr><tr><td>sparsity/dead_features_layer0</td><td>0</td></tr><tr><td>sparsity/dead_features_layer1</td><td>0</td></tr><tr><td>sparsity/dead_features_layer10</td><td>0</td></tr><tr><td>sparsity/dead_features_layer11</td><td>0</td></tr><tr><td>sparsity/dead_features_layer2</td><td>0</td></tr><tr><td>sparsity/dead_features_layer3</td><td>0</td></tr><tr><td>sparsity/dead_features_layer4</td><td>0</td></tr><tr><td>sparsity/dead_features_layer5</td><td>0</td></tr><tr><td>sparsity/dead_features_layer6</td><td>0</td></tr><tr><td>sparsity/dead_features_layer7</td><td>0</td></tr><tr><td>sparsity/dead_features_layer8</td><td>0</td></tr><tr><td>sparsity/dead_features_layer9</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer0</td><td>0.0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer1</td><td>0.0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer10</td><td>0.0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer11</td><td>0.0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer2</td><td>0.0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer3</td><td>0.0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer4</td><td>0.0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer5</td><td>0.0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer6</td><td>0.0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer7</td><td>0.0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer8</td><td>0.0</td></tr><tr><td>sparsity/mean_passes_since_fired_layer9</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">12288-L1-0.001-LR-0.0001-Tokens-5.000e+07</strong> at: <a href='https://wandb.ai/hufy-dev/sae_lens_tutorial/runs/x4a7nc5i' target=\"_blank\">https://wandb.ai/hufy-dev/sae_lens_tutorial/runs/x4a7nc5i</a><br/> View project at: <a href='https://wandb.ai/hufy-dev/sae_lens_tutorial' target=\"_blank\">https://wandb.ai/hufy-dev/sae_lens_tutorial</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240417_110242-x4a7nc5i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:x4a7nc5i). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270baf6f243040a8b1dd775b0d74dc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112087520046367, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hufy/SAELens/tutorials/wandb/run-20240417_110517-qyr8bs7m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hufy-dev/sae_lens_tutorial/runs/qyr8bs7m' target=\"_blank\">16384-L1-0.001-LR-0.0008-Tokens-5.000e+07</a></strong> to <a href='https://wandb.ai/hufy-dev/sae_lens_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hufy-dev/sae_lens_tutorial' target=\"_blank\">https://wandb.ai/hufy-dev/sae_lens_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hufy-dev/sae_lens_tutorial/runs/qyr8bs7m' target=\"_blank\">https://wandb.ai/hufy-dev/sae_lens_tutorial/runs/qyr8bs7m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective value: 3489830.0000:   2%|▏         | 2/100 [00:00<00:02, 36.56it/s]\n",
      "/home/hufy/SAELens/sae_lens/training/sparse_autoencoder.py:198: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m\n\u001b[1;32m      1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m LanguageModelSAERunnerConfig(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Data Generating Function (Model + Training Distibuion)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiny-stories-1L-21M\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# look at the next cell to see some instruction for what to do while this is running.\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m sparse_autoencoder_dictionary \u001b[38;5;241m=\u001b[39m \u001b[43mlanguage_model_sae_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/lm_runner.py:33\u001b[0m, in \u001b[0;36mlanguage_model_sae_runner\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     30\u001b[0m     wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mwandb_project, config\u001b[38;5;241m=\u001b[39mcast(Any, cfg), name\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mrun_name)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# train SAE\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m sparse_autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sae_on_language_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_autoencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivations_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_checkpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_sampling_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_sampling_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdead_feature_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdead_feature_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_to_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_log_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwandb_log_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mlog_to_wandb:\n\u001b[1;32m     46\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/train_sae_on_language_model.py:69\u001b[0m, in \u001b[0;36mtrain_sae_on_language_model\u001b[0;34m(model, sae_group, activation_store, batch_size, n_checkpoints, feature_sampling_window, dead_feature_threshold, use_wandb, wandb_log_frequency)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_sae_on_language_model\u001b[39m(\n\u001b[1;32m     56\u001b[0m     model: HookedTransformer,\n\u001b[1;32m     57\u001b[0m     sae_group: SparseAutoencoderDictionary,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     wandb_log_frequency: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     65\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SparseAutoencoderDictionary:\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    @deprecated Use `train_sae_group_on_language_model` instead. This method is kept for backward compatibility.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_sae_group_on_language_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43msae_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_checkpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_sampling_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_log_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msae_group\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/train_sae_on_language_model.py:117\u001b[0m, in \u001b[0;36mtrain_sae_group_on_language_model\u001b[0;34m(model, sae_group, activation_store, batch_size, n_checkpoints, feature_sampling_window, use_wandb, wandb_log_frequency)\u001b[0m\n\u001b[1;32m    114\u001b[0m checkpoint_paths: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n_training_tokens \u001b[38;5;241m<\u001b[39m total_training_tokens:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Do a training step.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     layer_acts \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     n_training_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m    120\u001b[0m     mse_losses: \u001b[38;5;28mlist\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/activations_store.py:393\u001b[0m, in \u001b[0;36mActivationsStore.next_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03mGet the next batch from the current DataLoader.\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;124;03mIf the DataLoader is exhausted, refill the buffer and create a new DataLoader.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# Try to get the next batch\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# If the DataLoader is exhausted, create a new one\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_data_loader()\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"tiny-stories-1L-21M\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n",
    "    hook_point=\"blocks.0.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n",
    "    hook_point_layer=0,  # Only one layer in the model.\n",
    "    d_in=1024,  # the width of the mlp output.\n",
    "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.\n",
    "    is_dataset_tokenized=True,\n",
    "    # SAE Parameters\n",
    "    mse_loss_normalization=None,  # We won't normalize the mse loss,\n",
    "    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.\n",
    "    b_dec_init_method=\"geometric_median\",  # The geometric median can be used to initialize the decoder weights.\n",
    "    # Training Parameters\n",
    "    lr=0.0008,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=10000,  # this can help avoid too many dead features initially.\n",
    "    l1_coefficient=0.001,  # will control how sparse the feature activations are\n",
    "    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
    "    train_batch_size=4096,\n",
    "    context_size=512,  # will control the lenght of the prompts we feed to the model. Larger is better but slower.\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n",
    "    total_training_tokens=1_000_000\n",
    "    * 50,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    store_batch_size=16,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    # WANDB\n",
    "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"sae_lens_tutorial\",\n",
    "    wandb_log_frequency=10,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "sparse_autoencoder_dictionary = language_model_sae_runner(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n",
      "Run name: 12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07\n",
      "n_tokens_per_buffer (millions): 0.262144\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.001024\n",
      "Total training steps: 24414\n",
      "Total wandb updates: 2441\n",
      "n_tokens_per_feature_sampling_window (millions): 524.288\n",
      "n_tokens_per_dead_feature_window (millions): 524.288\n",
      "We will reset the sparsity calculation 24 times.\n",
      "Number tokens in sparsity calculation window: 2.05e+06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhufy-dev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hufy/SAELens/tutorials/wandb/run-20240417_111458-h3rvry9l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hufy-dev/sae_lens_tutorial/runs/h3rvry9l' target=\"_blank\">12288-L1-1e-05-LR-0.0001-Tokens-5.000e+07</a></strong> to <a href='https://wandb.ai/hufy-dev/sae_lens_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hufy-dev/sae_lens_tutorial' target=\"_blank\">https://wandb.ai/hufy-dev/sae_lens_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hufy-dev/sae_lens_tutorial/runs/h3rvry9l' target=\"_blank\">https://wandb.ai/hufy-dev/sae_lens_tutorial/runs/h3rvry9l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective value: 2785086.5000:   4%|▍         | 4/100 [00:00<00:00, 127.95it/s]\n",
      "/home/hufy/SAELens/sae_lens/training/sparse_autoencoder.py:198: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n",
      "Objective value: 1531088.5000:   3%|▎         | 3/100 [00:00<00:00, 108.95it/s]\n",
      "Objective value: 2628265.7500:   4%|▍         | 4/100 [00:00<00:00, 119.53it/s]\n",
      "Objective value: 1799504.2500:   2%|▏         | 2/100 [00:00<00:01, 91.88it/s]\n",
      "Objective value: 1903927.7500:   2%|▏         | 2/100 [00:00<00:01, 92.58it/s]\n",
      "Objective value: 2208395.5000:   3%|▎         | 3/100 [00:00<00:00, 108.26it/s]\n",
      "Objective value: 2579191.0000:   2%|▏         | 2/100 [00:00<00:01, 90.38it/s]\n",
      "Objective value: 3092845.2500:   2%|▏         | 2/100 [00:00<00:01, 92.15it/s]\n",
      "Objective value: 3811167.0000:   2%|▏         | 2/100 [00:00<00:01, 92.67it/s]\n",
      "Objective value: 4869316.0000:   3%|▎         | 3/100 [00:00<00:00, 108.71it/s]\n",
      "Objective value: 7004212.5000:   3%|▎         | 3/100 [00:00<00:00, 109.73it/s]\n",
      "Objective value: 10393808.0000:   4%|▍         | 4/100 [00:00<00:00, 118.01it/s]\n",
      "99| MSE Loss 3.578 | L1 155.191:   0%|          | 202752/50000000 [00:29<1:36:20, 8615.41it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12288) must match the size of tensor b (256) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 48\u001b[0m\n\u001b[1;32m      1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m LanguageModelSAERunnerConfig(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Data Generating Function (Model + Training Distibuion)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# GPT2-small model\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# look at the next cell to see some instruction for what to do while this is running.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m sparse_autoencoder_dictionary \u001b[38;5;241m=\u001b[39m \u001b[43mlanguage_model_sae_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/lm_runner.py:33\u001b[0m, in \u001b[0;36mlanguage_model_sae_runner\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     30\u001b[0m     wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mwandb_project, config\u001b[38;5;241m=\u001b[39mcast(Any, cfg), name\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mrun_name)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# train SAE\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m sparse_autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sae_on_language_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_autoencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivations_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_checkpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_sampling_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_sampling_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdead_feature_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdead_feature_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_to_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwandb_log_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwandb_log_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mlog_to_wandb:\n\u001b[1;32m     46\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/train_sae_on_language_model.py:69\u001b[0m, in \u001b[0;36mtrain_sae_on_language_model\u001b[0;34m(model, sae_group, activation_store, batch_size, n_checkpoints, feature_sampling_window, dead_feature_threshold, use_wandb, wandb_log_frequency)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_sae_on_language_model\u001b[39m(\n\u001b[1;32m     56\u001b[0m     model: HookedTransformer,\n\u001b[1;32m     57\u001b[0m     sae_group: SparseAutoencoderDictionary,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     wandb_log_frequency: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     65\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SparseAutoencoderDictionary:\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    @deprecated Use `train_sae_group_on_language_model` instead. This method is kept for backward compatibility.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_sae_group_on_language_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43msae_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_checkpoints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_sampling_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwandb_log_frequency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msae_group\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/train_sae_on_language_model.py:156\u001b[0m, in \u001b[0;36mtrain_sae_group_on_language_model\u001b[0;34m(model, sae_group, activation_store, batch_size, n_checkpoints, feature_sampling_window, use_wandb, wandb_log_frequency)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (n_training_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m (wandb_log_frequency \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    155\u001b[0m                 sparse_autoencoder\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 156\u001b[0m                 \u001b[43mrun_evals\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msparse_autoencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mactivation_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mn_training_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_suffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m                 sparse_autoencoder\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# checkpoint if at checkpoint frequency\u001b[39;00m\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/evals.py:30\u001b[0m, in \u001b[0;36mrun_evals\u001b[0;34m(sparse_autoencoder, activation_store, model, n_training_steps, suffix)\u001b[0m\n\u001b[1;32m     27\u001b[0m eval_tokens \u001b[38;5;241m=\u001b[39m activation_store\u001b[38;5;241m.\u001b[39mget_batch_tokens()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Get Reconstruction Score\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m losses_df \u001b[38;5;241m=\u001b[39m \u001b[43mrecons_loss_batched\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43msparse_autoencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivation_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m recons_score \u001b[38;5;241m=\u001b[39m losses_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     38\u001b[0m ntp_loss \u001b[38;5;241m=\u001b[39m losses_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/evals.py:96\u001b[0m, in \u001b[0;36mrecons_loss_batched\u001b[0;34m(sparse_autoencoder, model, activation_store, n_batches)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_batches):\n\u001b[1;32m     95\u001b[0m     batch_tokens \u001b[38;5;241m=\u001b[39m activation_store\u001b[38;5;241m.\u001b[39mget_batch_tokens()\n\u001b[0;32m---> 96\u001b[0m     score, loss, recons_loss, zero_abl_loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_recons_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43msparse_autoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_tokens\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    100\u001b[0m         (\n\u001b[1;32m    101\u001b[0m             score\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m         )\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    108\u001b[0m losses \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    109\u001b[0m     losses, columns\u001b[38;5;241m=\u001b[39mcast(Any, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecons_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzero_abl_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    110\u001b[0m )\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/evals.py:156\u001b[0m, in \u001b[0;36mget_recons_loss\u001b[0;34m(sparse_autoencoder, model, batch_tokens)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     replacement_hook \u001b[38;5;241m=\u001b[39m standard_replacement_hook\n\u001b[0;32m--> 156\u001b[0m recons_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_hooks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfwd_hooks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplacement_hook\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m zero_abl_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrun_with_hooks(\n\u001b[1;32m    163\u001b[0m     batch_tokens, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, fwd_hooks\u001b[38;5;241m=\u001b[39m[(hook_point, zero_ablate_hook)]\n\u001b[1;32m    164\u001b[0m )\n\u001b[1;32m    166\u001b[0m score \u001b[38;5;241m=\u001b[39m (zero_abl_loss \u001b[38;5;241m-\u001b[39m recons_loss) \u001b[38;5;241m/\u001b[39m (zero_abl_loss \u001b[38;5;241m-\u001b[39m loss)\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/transformer_lens/hook_points.py:365\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: Hooks will be reset at the end of run_with_hooks. This removes the backward hooks before a backward pass can occur.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m     )\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    363\u001b[0m     fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts\n\u001b[1;32m    364\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m hooked_model:\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhooked_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/transformer_lens/HookedTransformer.py:569\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    565\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    566\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    567\u001b[0m         )\n\u001b[0;32m--> 569\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/transformer_lens/components.py:1464\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     mlp_in \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1459\u001b[0m         resid_mid\n\u001b[1;32m   1460\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_hook_mlp_in\n\u001b[1;32m   1461\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_mlp_in(resid_mid\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m   1462\u001b[0m     )\n\u001b[1;32m   1463\u001b[0m     normalized_resid_mid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(mlp_in)\n\u001b[0;32m-> 1464\u001b[0m     mlp_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_mlp_out\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_resid_mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m     resid_post \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_resid_post(\n\u001b[1;32m   1468\u001b[0m         resid_mid \u001b[38;5;241m+\u001b[39m mlp_out\n\u001b[1;32m   1469\u001b[0m     )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mparallel_attn_mlp:\n\u001b[1;32m   1471\u001b[0m     \u001b[38;5;66;03m# Dumb thing done by GPT-J, both MLP and Attn read from resid_pre and write to resid_post, no resid_mid used.\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;66;03m# In GPT-J, LN1 and LN2 are tied, in GPT-NeoX they aren't.\u001b[39;00m\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1581\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1581\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1584\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/SAELens/.venv/lib/python3.11/site-packages/transformer_lens/hook_points.py:65\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/evals.py:126\u001b[0m, in \u001b[0;36mget_recons_loss.<locals>.standard_replacement_hook\u001b[0;34m(activations, hook)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstandard_replacement_hook\u001b[39m(activations: torch\u001b[38;5;241m.\u001b[39mTensor, hook: Any):\n\u001b[0;32m--> 126\u001b[0m     activations \u001b[38;5;241m=\u001b[39m \u001b[43msparse_autoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msae_out\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    127\u001b[0m         activations\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m activations\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/sparse_autoencoder.py:181\u001b[0m, in \u001b[0;36mSparseAutoencoder.forward\u001b[0;34m(self, x, dead_neuron_mask)\u001b[0m\n\u001b[1;32m    172\u001b[0m     ghost_grad_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalculate_ghost_grad_loss(\n\u001b[1;32m    173\u001b[0m         x\u001b[38;5;241m=\u001b[39mx,\n\u001b[1;32m    174\u001b[0m         sae_out\u001b[38;5;241m=\u001b[39msae_out,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m         dead_neuron_mask\u001b[38;5;241m=\u001b[39mdead_neuron_mask,\n\u001b[1;32m    178\u001b[0m     )\n\u001b[1;32m    180\u001b[0m mse_loss \u001b[38;5;241m=\u001b[39m per_item_mse_loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m--> 181\u001b[0m sparsity \u001b[38;5;241m=\u001b[39m \u001b[43m_per_item_sparsity_loss_with_target_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_acts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlp_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_loss_normalization\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,))\n\u001b[1;32m    184\u001b[0m l1_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_coefficient \u001b[38;5;241m*\u001b[39m sparsity\n\u001b[1;32m    185\u001b[0m loss \u001b[38;5;241m=\u001b[39m mse_loss \u001b[38;5;241m+\u001b[39m l1_loss \u001b[38;5;241m+\u001b[39m ghost_grad_loss\n",
      "File \u001b[0;32m~/SAELens/sae_lens/training/sparse_autoencoder.py:434\u001b[0m, in \u001b[0;36m_per_item_sparsity_loss_with_target_norm\u001b[0;34m(feature_acts, target, lp_norm, l1_loss_normalization)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m l1_loss_normalization \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2_squared\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    433\u001b[0m     normalization \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeature_acts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlp_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnormalization\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feature_acts\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39mlp_norm, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (12288) must match the size of tensor b (256) at non-singleton dimension 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99| MSE Loss 3.578 | L1 155.191:   0%|          | 202752/50000000 [00:42<1:36:20, 8615.41it/s]"
     ]
    }
   ],
   "source": [
    "cfg = LanguageModelSAERunnerConfig(\n",
    "    # Data Generating Function (Model + Training Distibuion)\n",
    "    model_name=\"gpt2\",  # GPT2-small model\n",
    "    hook_point=\"blocks.{layer}.hook_mlp_out\",  # MLP output of each layer\n",
    "    hook_point_layer=[int(i) for i in range(12)],  # All layers\n",
    "    d_in=768,  # the width of the mlp output.\n",
    "    dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.\n",
    "    is_dataset_tokenized=True,\n",
    "    # SAE Parameters\n",
    "    mse_loss_normalization=None,  # We won't normalize the mse loss,\n",
    "    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.\n",
    "    b_dec_init_method=\"geometric_median\",  # The geometric median can be used to initialize the decoder weights.\n",
    "    decoder_orthogonal_init = True, # Orthogonal initialization of the decoder weights\n",
    "    sae_type = \"tanh_sae\", # Use tanh activation for the SAE\n",
    "    noise_scale = 1, # Add noise to the input for tanh SAE to avoid activation collapse\n",
    "    # Training Parameters\n",
    "    lr=0.0001,  # lower the better, we'll go fairly high to speed up the tutorial.\n",
    "    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n",
    "    lr_warm_up_steps=10000,  # this can help avoid too many dead features initially.\n",
    "    l1_coefficient=0.0001,  # will control how sparse the feature activations are\n",
    "    l1_loss_normalization=\"l2_squared\", # L2 squared normalization for the L1 loss\n",
    "    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n",
    "    train_batch_size=2048,\n",
    "    context_size=256,  # will control the lenght of the prompts we feed to the model. Larger is better but slower.\n",
    "    # Activation Store Parameters\n",
    "    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n",
    "    total_training_tokens=1_000_000\n",
    "    * 50,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n",
    "    store_batch_size=16,\n",
    "    # Resampling protocol\n",
    "    use_ghost_grads=False,\n",
    "    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n",
    "    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n",
    "    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n",
    "    # WANDB\n",
    "    log_to_wandb=True,  # always use wandb unless you are just testing code.\n",
    "    wandb_project=\"sae_lens_tutorial\",\n",
    "    wandb_log_frequency=10,\n",
    "    # Misc\n",
    "    device=device,\n",
    "    seed=42,\n",
    "    n_checkpoints=0,\n",
    "    checkpoint_path=\"checkpoints\",\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# look at the next cell to see some instruction for what to do while this is running.\n",
    "sparse_autoencoder_dictionary = language_model_sae_runner(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO: Understanding TinyStories-1L with our SAE\n",
    "\n",
    "I haven't had time yet to complete this section, but I'd love to see a PR where someones uses an SAE they trained in this tutorial to understand this model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>16211</th>\n",
       "      <th>9873</th>\n",
       "      <th>7694</th>\n",
       "      <th>13782</th>\n",
       "      <th>9370</th>\n",
       "      <th>7699</th>\n",
       "      <th>9477</th>\n",
       "      <th>7443</th>\n",
       "      <th>227</th>\n",
       "      <th>6926</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>checking</td>\n",
       "      <td>Restore</td>\n",
       "      <td>cere</td>\n",
       "      <td>In</td>\n",
       "      <td>speakers</td>\n",
       "      <td>ju</td>\n",
       "      <td>32</td>\n",
       "      <td>ow</td>\n",
       "      <td>bum</td>\n",
       "      <td>owner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Berry</td>\n",
       "      <td>telling</td>\n",
       "      <td>uc</td>\n",
       "      <td>Kn</td>\n",
       "      <td>roller</td>\n",
       "      <td>Boot</td>\n",
       "      <td>insert</td>\n",
       "      <td>o</td>\n",
       "      <td>?!\"</td>\n",
       "      <td>clerk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Squ</td>\n",
       "      <td>ruce</td>\n",
       "      <td>lesson</td>\n",
       "      <td>grand</td>\n",
       "      <td>moss</td>\n",
       "      <td>drunk</td>\n",
       "      <td>improve</td>\n",
       "      <td>stop</td>\n",
       "      <td>simply</td>\n",
       "      <td>window</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reason</td>\n",
       "      <td>observ</td>\n",
       "      <td>debate</td>\n",
       "      <td>Today</td>\n",
       "      <td>escal</td>\n",
       "      <td>upstream</td>\n",
       "      <td>xious</td>\n",
       "      <td>e</td>\n",
       "      <td>sheet</td>\n",
       "      <td>keeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bro</td>\n",
       "      <td>erc</td>\n",
       "      <td>minor</td>\n",
       "      <td>Jay</td>\n",
       "      <td>mac</td>\n",
       "      <td>,'</td>\n",
       "      <td>Todd</td>\n",
       "      <td>ah</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>owner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>merge</td>\n",
       "      <td>gracious</td>\n",
       "      <td>mentioned</td>\n",
       "      <td>Finally</td>\n",
       "      <td>hy</td>\n",
       "      <td>traffic</td>\n",
       "      <td>Mor</td>\n",
       "      <td>you</td>\n",
       "      <td>double</td>\n",
       "      <td>keepers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Las</td>\n",
       "      <td>self</td>\n",
       "      <td>succeeding</td>\n",
       "      <td>Then</td>\n",
       "      <td>bumper</td>\n",
       "      <td>Soldier</td>\n",
       "      <td>allergy</td>\n",
       "      <td>wo</td>\n",
       "      <td>Joyce</td>\n",
       "      <td>worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>angling</td>\n",
       "      <td>regretted</td>\n",
       "      <td>national</td>\n",
       "      <td>Family</td>\n",
       "      <td>salad</td>\n",
       "      <td>pitched</td>\n",
       "      <td>Phillip</td>\n",
       "      <td>hey</td>\n",
       "      <td>fantastic</td>\n",
       "      <td>manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>slid</td>\n",
       "      <td>sense</td>\n",
       "      <td>fines</td>\n",
       "      <td>During</td>\n",
       "      <td>hooks</td>\n",
       "      <td>narrator</td>\n",
       "      <td>itability</td>\n",
       "      <td>whe</td>\n",
       "      <td>pron</td>\n",
       "      <td>employee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>blast</td>\n",
       "      <td>restoration</td>\n",
       "      <td>ul</td>\n",
       "      <td>H</td>\n",
       "      <td>rings</td>\n",
       "      <td>vegetarian</td>\n",
       "      <td>aback</td>\n",
       "      <td>Ow</td>\n",
       "      <td>Dahl</td>\n",
       "      <td>employees</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      16211         9873         7694     13782      9370         7699   \\\n",
       "0  checking       Restore         cere       In   speakers           ju   \n",
       "1     Berry       telling           uc       Kn     roller         Boot   \n",
       "2       Squ          ruce       lesson    grand       moss        drunk   \n",
       "3    reason        observ       debate    Today      escal     upstream   \n",
       "4       Bro           erc        minor      Jay        mac           ,'   \n",
       "5     merge      gracious    mentioned  Finally         hy      traffic   \n",
       "6       Las          self   succeeding     Then     bumper      Soldier   \n",
       "7   angling     regretted     national   Family      salad      pitched   \n",
       "8      slid         sense        fines   During      hooks     narrator   \n",
       "9     blast   restoration           ul        H      rings   vegetarian   \n",
       "\n",
       "       9477   7443        227         6926   \n",
       "0         32     ow         bum       owner  \n",
       "1     insert      o         ?!\"       clerk  \n",
       "2    improve   stop      simply      window  \n",
       "3      xious      e       sheet      keeper  \n",
       "4       Todd     ah   Charlotte       owner  \n",
       "5        Mor    you      double     keepers  \n",
       "6    allergy     wo       Joyce      worker  \n",
       "7    Phillip    hey   fantastic     manager  \n",
       "8  itability    whe        pron    employee  \n",
       "9      aback     Ow        Dahl   employees  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Let's start by getting the top 10 logits for each feature\n",
    "\n",
    "sparse_autoencoder = next(iter(sparse_autoencoder_dictionary))[1]\n",
    "projection_onto_unembed = sparse_autoencoder.W_dec @ model.W_U\n",
    "\n",
    "\n",
    "# get the top 10 logits.\n",
    "vals, inds = torch.topk(projection_onto_unembed, 10, dim=1)\n",
    "\n",
    "# get 10 random features\n",
    "random_indices = torch.randint(0, projection_onto_unembed.shape[0], (10,))\n",
    "\n",
    "# Show the top 10 logits promoted by those features\n",
    "top_10_logits_df = pd.DataFrame(\n",
    "    [model.to_str_tokens(i) for i in inds[random_indices]],\n",
    "    index=random_indices.tolist(),\n",
    ").T\n",
    "top_10_logits_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mats_sae_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
